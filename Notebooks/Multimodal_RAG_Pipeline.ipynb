{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPVQadklhtN2"
      },
      "outputs": [],
      "source": [
        "! pip install openai langchain pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install unstructured"
      ],
      "metadata": {
        "id": "3VMU0WcopW_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "tKZES9x4hzP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install filestack-python"
      ],
      "metadata": {
        "id": "T2xi5blhbk79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWiGnG7Qn5Dc",
        "outputId": "d8ce7551-1453-4df2-ac90-b23339e1450f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pinecone\n",
        "import torch\n",
        "import os\n",
        "from pkg_resources import packaging\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "import hashlib\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain.document_loaders import UnstructuredHTMLLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import uuid\n",
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "metadata": {
        "id": "FnxID-b3kgpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import CLIP (Contrastive Languageâ€“Image Pre-training)"
      ],
      "metadata": {
        "id": "18tsEqqVm49s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ],
      "metadata": {
        "id": "tOUp1e65m4Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# please change this to CUDA when you have the GPU\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# we will be using ViT-B/32 model\n",
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "model = model.to(DEVICE)\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "id": "rxtwWIzHnIZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main image path\n",
        "MAIN_PATH = \"<PATH TO IMAGES\"\n",
        "\n",
        "# create an image dictionary\n",
        "image_dict = {}\n",
        "\n",
        "for image_file in os.listdir(MAIN_PATH):\n",
        "    # get the image path\n",
        "    image_path = os.path.join(MAIN_PATH, image_file)\n",
        "    image_dict[image_file] = {\n",
        "        \"filename\": image_file,\n",
        "        \"path\": image_path\n",
        "    }"
      ],
      "metadata": {
        "id": "AXsqX99nnOTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload files to filestack to get the URL"
      ],
      "metadata": {
        "id": "0WihFbwO6VqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from filestack import Client\n",
        "client = Client(\"YOUR FILESTACK API KEY\")\n",
        "\n",
        "\n",
        "def upload_to_filestack(path):\n",
        "\n",
        "    store_params = {\n",
        "        \"mimetype\": \"image/png\"\n",
        "    }\n",
        "    new_filelink = client.upload(filepath=path, store_params=store_params)\n",
        "    print(new_filelink.url)\n",
        "    return new_filelink.url"
      ],
      "metadata": {
        "id": "CDJmNGHnb1jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Embeddings"
      ],
      "metadata": {
        "id": "rCoEf3PAEi2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dictionary = {}\n",
        "for file_name in tqdm(image_dict):\n",
        "    file_path = image_dict[file_name][\"path\"]\n",
        "    upload_url = upload_to_filestack(file_path)\n",
        "    try:\n",
        "        # preprocessing is compulsory here\n",
        "        preprocess_image = preprocess(Image.open(file_path).convert(\"RGB\")).unsqueeze(0).to(DEVICE)\n",
        "        encoddings = model.encode_image(preprocess_image).tolist()[0]\n",
        "    except Exception as error:\n",
        "        message = \"Cannot encode the image, err: {}\".format(str(error))\n",
        "        print(message)\n",
        "        continue\n",
        "    # set the encoding dictionary\n",
        "    embedding_dictionary[file_name] = {\n",
        "        'name': file_name,\n",
        "        'path': upload_url,\n",
        "        'embeddings': encoddings\n",
        "    }"
      ],
      "metadata": {
        "id": "yGpn9ZGksgBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload Image embeddings to pinecone"
      ],
      "metadata": {
        "id": "2gFybICnva63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up pinecone environment\n",
        "os.environ['PINECONE_API_KEY'] = \"\"\n",
        "os.environ['PINECONE_API_ENV'] = \"\"\n",
        "os.environ['PINECONE_INDEX_NAME'] = \"\"\n",
        "# set index\n",
        "pinecone.init( api_key=os.environ['PINECONE_API_KEY'], environment=os.environ['PINECONE_API_ENV'])\n",
        "pinecone_index=pinecone.Index(os.environ['PINECONE_INDEX_NAME'])"
      ],
      "metadata": {
        "id": "5CvBLkRcuyEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload to pinecone\n",
        "for image in embedding_dictionary:\n",
        "    image_name = embedding_dictionary[image]['name']\n",
        "    path = embedding_dictionary[image]['path']\n",
        "    embeddings = embedding_dictionary[image]['embeddings']\n",
        "\n",
        "    # pinecone settings\n",
        "    document_hash = hashlib.md5(path.encode(\"utf-8\"))\n",
        "    metadata = {\"image_name\": image_name, \"file_path\": path}\n",
        "    pinecone_index.upsert([(document_hash.hexdigest(), embeddings, metadata)])\n",
        "    print(\"{}===>Added\".format(image_name))\n"
      ],
      "metadata": {
        "id": "N-pWgY6-vdny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload Text Embeddings to Pinecone"
      ],
      "metadata": {
        "id": "sFkH-EFvod1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This is different pinecone setup"
      ],
      "metadata": {
        "id": "zVE0CBH-rpIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If you do have pinecone paid version, you can create another index which makes the process much easier"
      ],
      "metadata": {
        "id": "mcWL1vNa6u7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up pinecone environment\n",
        "os.environ['MY_PINECONE_API_KEY'] = \"\"\n",
        "os.environ['MY_PINECONE_API_ENV'] = \"\"\n",
        "os.environ['MY_PINECONE_INDEX_NAME'] = \"\"\n",
        "# set index\n",
        "pinecone.init( api_key=os.environ['MY_PINECONE_API_KEY'], environment=os.environ['MY_PINECONE_API_ENV'])\n",
        "pinecone_index_text=pinecone.Index(os.environ['MY_PINECONE_INDEX_NAME'])"
      ],
      "metadata": {
        "id": "I7WJY1G-roa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_PATH = \"backyard-birds.html\""
      ],
      "metadata": {
        "id": "_Am6BagspMb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the html document\n",
        "loader = UnstructuredHTMLLoader(TEXT_PATH)\n",
        "text_info = loader.load()\n",
        "text_file = text_info[0].page_content\n",
        "print(\"Number of documents: {}\".format(len(text_info)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2NtRuuqohlG",
        "outputId": "653c993a-3ee3-4d97-d366-942b6763104a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the texts\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "        # Set a really small chunk size, just to show.\n",
        "        chunk_size = 1000,\n",
        "        chunk_overlap  = 20,\n",
        "        length_function = len,\n",
        "        is_separator_regex = False,\n",
        "    )\n",
        "    # text splitter\n",
        "texts = text_splitter.create_documents([text_file])"
      ],
      "metadata": {
        "id": "ICqkStqKqO1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the openai key\n",
        "openai.api_key = \"OPEANAI_API_KEY\""
      ],
      "metadata": {
        "id": "xYgWD6EBtbWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embeddings model\n",
        "MODEL = \"text-embedding-ada-002\"\n",
        "\n",
        "for index, sub_docs in enumerate(texts):\n",
        "    document_hash = hashlib.md5(sub_docs.page_content.encode(\"utf-8\"))\n",
        "    embedding = openai.embeddings.create(model= MODEL,input=sub_docs.page_content).data[0].embedding\n",
        "    metadata = {\"chunk\": str(uuid.uuid4()), \"text\": sub_docs.page_content, \"doc_index\":index}\n",
        "    pinecone_index_text.upsert([(document_hash.hexdigest(), embedding, metadata)])\n",
        "    print(\"{} ==> Done\".format(index))"
      ],
      "metadata": {
        "id": "HlkMpZNXrU7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NOTE\n",
        "\n",
        "- If you use two seperate pinecone setups, you cannot run the both queries. Try to run it independently, then it will work\n",
        "- Or create seperate scripts. Below things are for the reference."
      ],
      "metadata": {
        "id": "V0GUCot37ILX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Query"
      ],
      "metadata": {
        "id": "GgzeVC1z1V_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def qa_engine(question):\n",
        "    # pinecone env\n",
        "    index=pinecone.Index(os.environ['MY_PINECONE_INDEX_NAME'])\n",
        "\n",
        "    question_embed_call = openai.embeddings.create(input = question ,model = MODEL)\n",
        "    query_embeds = question_embed_call.data[0].embedding\n",
        "    response = index.query(query_embeds,top_k = 1,include_metadata = True)\n",
        "    # get the response text and metadata\n",
        "    response = response[\"matches\"][0][\"metadata\"]\n",
        "    text = response.get(\"text\", \"UNKNOWN\")\n",
        "    chunk = response.get(\"chunk\", \"UNKNOWN\")\n",
        "    doc_index = response.get(\"doc_index\", \"UNKNOWN\")\n",
        "    offset=\", OFFSET=\"+str(response.get(\"chunk\",\"UNKNOWN\"))\n",
        "\n",
        "    # query document\n",
        "    query_doc = []\n",
        "\n",
        "    # create metadata for q&a chain\n",
        "    metadata = {\n",
        "        \"id\": chunk,\n",
        "        \"filename\": doc_index,\n",
        "        \"source\": str(doc_index) + offset\n",
        "    }\n",
        "\n",
        "    query_doc.append(Document(page_content=text, metadata = metadata))\n",
        "\n",
        "    # query the answer from llm\n",
        "    llm = OpenAI(temperature=0, openai_api_key = openai.api_key)\n",
        "    chain = load_qa_with_sources_chain(llm, verbose = False)\n",
        "    # get the chain response\n",
        "    chain_response = chain.run(input_documents = query_doc, question = question )\n",
        "    print(chain_response)\n",
        "\n",
        "\n",
        "qa_engine(\"Where does American Goldfinch's yellow hue come from?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjoIbIir1YIZ",
        "outputId": "288207cc-28ca-4424-80ad-b2f9ae02c664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The American Goldfinch's yellow hue comes from pigments called carotenoids.\n",
            "SOURCES: 6.0, OFFSET=363f5aaa-524e-4a53-acdd-d28c6e96bea4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_engine(\"What is American Goldfinch's\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilkd7FcwAh4z",
        "outputId": "406286ab-418f-49f4-882a-6df76d6cd4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The American Goldfinch is a small bird with a yellow plumage. It primarily eats seeds from plants in the aster family, such as thistles and sunflowers.\n",
            "SOURCES: 6.0, OFFSET=363f5aaa-524e-4a53-acdd-d28c6e96bea4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Query"
      ],
      "metadata": {
        "id": "wVEfyuV_xADX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_images(query):\n",
        "    # get the tekenizers\n",
        "    tokens = clip.tokenize(query).to(DEVICE)\n",
        "    query_embeds = model.encode_text(tokens).tolist()[0]\n",
        "    response = pinecone_index_image.query(query_embeds,top_k = 1,include_metadata = True)\n",
        "    file_path = response['matches'][0]['metadata']['file_path']\n",
        "    image_name = response['matches'][0]['metadata']['image_name']\n",
        "    score = response['matches'][0]['score']\n",
        "    print(\"Image Name: {}\".format(image_name))\n",
        "    # open the image\n",
        "    plt.figure(figsize = (5,5))\n",
        "    plt.imshow(Image.open(file_path).convert(\"RGB\"))\n",
        "    plt.title(\"confidence Score: {}\".format(score))\n",
        "    plt.axis('off');\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PFFxsZjk2ME2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = \"Where does American Goldfinch's yellow hue come from?\"\n",
        "query_images(QUERY)"
      ],
      "metadata": {
        "id": "o95TIYB53AB0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}